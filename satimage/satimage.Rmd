---
title: "Satimage"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
---


```{r}
library(ggplot2); # For some graphs
library(FactoMineR); # For PCA
library(factoextra); # For PCA
library(caret); # For feature plots
library(e1071); # For SVM
library(nnet); # For Neural nets
library(pROC); # For ROC
library(multiROC) # For multiclass ROC

library(cluster) # For k-means
library(fpc) # For k-means
library(seriation) # For k-means
library(ggpubr) # For clustering visualization
library(TSclust) # For clustering Evaluation

library(knitr); # For table printing
```

```{r}
# Create a vector containing all attribute names
attribute.names = c()
for(pixel.number in 1:9) {
  for(attribute.suffix in c("Red", "Green", "NIR1", "NIR2")) {
    attribute.name = paste("p", pixel.number, ".", attribute.suffix, sep="")
    attribute.names = c(attribute.names, attribute.name)
  }
}
attribute.names = c(attribute.names, "class")

# Create a vector containing all class names
class.names = c("red soil", "cotton crop", "grey soil", "damp grey soil", "soil with vegetation stubble", "very damp grey soil")

# Load the dataset from the csv file and cast the label to factor
dataset = data.frame(read.csv("dataset.csv", col.names = attribute.names, sep=" "))
dataset$class = factor(dataset$class, labels = class.names)

# Determine the input and the output space
dataset.input = dataset[, 1:ncol(dataset)-1]
dataset.output = dataset$class
```

```{r}
# Determine some statistical measures on the input attributes
dataset.mean = sapply(dataset.input, mean)
dataset.sd = sapply(dataset.input, sd)
dataset.var = sapply(dataset.input, var)
dataset.min = sapply(dataset.input, min)
dataset.max = sapply(dataset.input, max)
dataset.median = sapply(dataset.input, median)
dataset.range = sapply(dataset.input, range)
dataset.quantile = sapply(dataset.input, quantile)

```

```{r}
# Determine class frequencies
class.frequencies = data.frame(table(dataset.output))
colnames(class.frequencies) = c("class", "frequencies")
class.frequencies$percentages = paste(format((class.frequencies$frequencies / sum(class.frequencies$frequencies)) * 100, digits = 2), "%", sep = "")
ggplot(data = class.frequencies, aes(x = class, y = frequencies, fill = class)) + geom_bar(stat = "identity") + geom_text(aes(label = percentages), vjust = 1.5, colour = "white")
ggplot(data = class.frequencies, aes(x = "", y = frequencies, fill = class)) + geom_bar(stat = "identity", width = 1, color = "white") + coord_polar("y", start = 0)
```
## Separabilità
I valori relativi allo stesso canale tra pixel adiacenti risultano correlati (come evidente dai grafici), questo perche' il dataset e' costruito come una sliding window quadrata 3x3 su una singola immagine satellitare. Ci si aspetta quindi una varianza piuttosto contenuta sui singoli valori e la sopra citata correlazione.

TODO: Aggiungere titoli ai featureplot
```{r}
featurePlot(dataset.input[, c("p5.Red", "p5.Green", "p5.NIR1")], dataset.output, plot="pairs", scales=list((relation="free"), y=list(relation="free")), auto.key=list(columns=3))
featurePlot(dataset.input[, c("p5.Red", "p6.Red", "p4.Red")], dataset.output, plot="pairs", scales=list((relation="free"), y=list(relation="free")), auto.key=list(columns=3))
featurePlot(dataset.input[, c("p5.Green", "p6.Green", "p4.Green")], dataset.output, plot="pairs", scales=list((relation="free"), y=list(relation="free")), auto.key=list(columns=3))
featurePlot(dataset.input[, c("p5.Green", "p2.Green", "p8.Green")], dataset.output, plot="pairs", scales=list((relation="free"), y=list(relation="free")), auto.key=list(columns=3))
featurePlot(dataset.input[, c("p5.Red", "p2.Red", "p8.Red")], dataset.output, plot="pairs", scales=list((relation="free"), y=list(relation="free")), auto.key=list(columns=3))
featurePlot(dataset.input[, c("p5.NIR1", "p2.NIR1", "p8.NIR1")], dataset.output, plot="pairs", scales=list((relation="free"), y=list(relation="free")), auto.key=list(columns=3))
```

## PCA
Data la correlazione rilevata precedentemente e l'alto numero di attributi in input sembra più che opportuno eseguire una PCA sul dataset al fine di ridurre la dimensione dello spazio di input.
```{r}
# Execute the PCA. Extract only the first four principal components
pca.results <- PCA(dataset.input, scale.unit = TRUE, ncp = 4, graph = FALSE)

# Get the eigenvalues and plot them. The first four components explain about 92% of variance
pca.results.eig.val <- get_eigenvalue(pca.results)
fviz_eig(pca.results, addlabels = TRUE, ylim = c(0, 50))

# Show the correlation between the variables using the first two principal components
# Many variable are positively correlated
fviz_pca_var(pca.results, col.var = "black")

```

Analizzando le coordinate per i singoli attributi si riesce a capire i 4 cluster rappresentano i 4 tipi di variabili in gioco (Red, Green NIR1, NIR2)

```{r}
# Extract the principal components
dataset.input.pca = data.frame(get_pca_ind(pca.results)$coord)
dataset.pca = cbind(class=dataset$class, dataset.input.pca)

# Draw some feature plot to highlight the level of difficulty to recognise the various classes
featurePlot(x=dataset.input.pca, y=dataset.output, plot="density", scales=list(x=list(relation="free"), y=list(relation="free")), auto.key=list(columns=3))
featurePlot(x=dataset.input.pca, y=dataset.output, plot="pairs", auto.key=list(columns=3))
```


```{r}
# Divide the dataset into trainset and testset
indexes = sample(2, nrow(dataset.pca), replace = TRUE, prob = c(0.7, 0.3))
temp.trainset = dataset.pca[indexes == 1,]
testset = dataset.pca[indexes == 2,]
testset.x = testset[, !names(testset) %in% c("class")]
testset.y = testset[,c("class")]

```


```{r}
# Divide the trainset to obtain a validationset
indexes = sample(2, nrow(temp.trainset), replace = TRUE, prob = c(0.9, 0.1))
trainset = temp.trainset[indexes == 1,]
validationset = temp.trainset[indexes == 2,]
validationset.x = validationset[,!names(validationset) %in% c("class")]
validationset.y =  validationset[,c("class")]
```

## SVM Training

```{r}
svm.tuning.control = tune.control(sampling = "fix")
svm.tuning = tune.svm(class ~ ., data = trainset, kernel = "radial", gamma = 10^(-3:1), cost = 10^(-2:1), validation.x = validationset[,!names(validationset) %in% c("class")], validation.y = validationset[,c("class")], tunecontrol = svm.tuning.control, probability = T)
plot(svm.tuning)
```


```{r}
svm.tuned = svm.tuning$best.model
svm.tuned.validation.prediction = predict(svm.tuned, validationset.x)
svm.tuned.validation.confusion_matrix = table(svm.tuned.validation.prediction, validationset.y)
svm.tuned.accuracy = sum(diag(svm.tuned.validation.confusion_matrix)) / sum(svm.tuned.validation.confusion_matrix)
sprintf("Tuned SVM accuracy is %2.1f%%", (1 - svm.tuning$best.performance) * 100)
sprintf("Best SVM parameters were gamma: %d cost: %d", svm.tuning$best.parameters$gamma, svm.tuning$best.parameters$cost)
print(svm.tuned.validation.confusion_matrix)
```

```{r}
nnet.tuning.control = tune.control(sampling = 'fix', performances = T)
nnet.tuning = tune.nnet(class ~., data = trainset, size = c(2:4), validation.x = validationset.x, validation.y = validationset.y, decay = c(0, 2^-2:1))
nnet.tuned = nnet.tuning$best.model
plot(nnet.tuning)
```

```{r}
acc = 1 - nnet.tuning$best.performance
sprintf("Tuned neuralnet overall accuracy is %2.1f%%", acc * 100)
sprintf("Tuned neuralnet best parameters were size: %d decay: %d", nnet.tuning$best.parameters$size, nnet.tuning$best.parameters$decay)
```

```{r}
predictions = predict(nnet.tuning$best.model, testset.x)
predictions.classes = class.names[max.col(predictions)]
errors = predictions.classes != testset.y
acc = length(which(errors))/length(errors)
nnet.tuned.validation.predictions = predictions.classes
nnet.tuned.validation.confusion_matrix = table(nnet.tuned.validation.predictions, testset.y)
print(nnet.tuned.validation.confusion_matrix)
```

## Confronto modelli

```{r}
svm.tuned.validation.predictions = predict(svm.tuned, validationset.x, probability = T)
svm.tuned.validation.predictions.probs = attr(svm.tuned.validation.predictions, "probabilities")
svm.roc = multiclass.roc(validationset.y, svm.tuned.validation.predictions.probs)

```

```{r}
nnet.tuned.validation.predictions = predict(nnet.tuned, validationset.x, probability = T)
nnet.roc = multiclass.roc(validationset.y, nnet.tuned.validation.predictions)
```

```{r}
sprintf("SVM ROC AUC: %f", svm.roc$auc)
sprintf("NNET ROC AUC: %f", nnet.roc$auc)
```

```{r}
# MultiROC

validationset.multiroc = data.frame(matrix(ncol=18, nrow=0))

for (i in 1:length(validationset.y)) {
    row = matrix(0, 1, 6)
    row[validationset.y[i]] = 1
    for (j in 1:6)
        row = c(row, svm.tuned.validation.predictions.probs[i, class.names[j]])
    row = c(row, nnet.tuned.validation.predictions[i,])
    validationset.multiroc = rbind(validationset.multiroc, row)
}
colnames(validationset.multiroc) = c('red_true', 'cotton_true', 'grey_true', 'damp grey_true', 'vegetation_true', 'very damp grey_true',
                                     'red_pred_SVM', 'cotton_pred_SVM', 'grey_pred_SVM', 'damp grey_pred_SVM', 'vegetation_pred_SVM', 'very damp grey_pred_SVM',
                                     'red_pred_NN', 'cotton_pred_NN', 'grey_pred_NN', 'damp grey_pred_NN', 'vegetation_pred_NN', 'very damp grey_pred_NN')

svm.nnet.multiroc <- multi_roc(validationset.multiroc, force_diag=T)
unlist(svm.nnet.multiroc$AUC)

```


```{r}
# MultiROC plotting

n_method <- length(unique(svm.nnet.multiroc$Methods))
n_group <- length(unique(svm.nnet.multiroc$Groups))
res_df <- data.frame(Specificity= numeric(0), Sensitivity= numeric(0), Group = character(0), AUC = numeric(0), Method = character(0))

for (i in 1:n_method) {
      for (j in 1:n_group) {
        temp_data_1 <- data.frame(Specificity=svm.nnet.multiroc$Specificity[[i]][j],
                                  Sensitivity=svm.nnet.multiroc$Sensitivity[[i]][j],
                                  Group=unique(svm.nnet.multiroc$Groups)[j],
                                  AUC=svm.nnet.multiroc$AUC[[i]][j],
                                  Method = unique(svm.nnet.multiroc$Methods)[i])
        colnames(temp_data_1) <- c("Specificity", "Sensitivity", "Group", "AUC", "Method")
        res_df <- rbind(res_df, temp_data_1)

      }
      temp_data_2 <- data.frame(Specificity=svm.nnet.multiroc$Specificity[[i]][n_group+1],
                                Sensitivity=svm.nnet.multiroc$Sensitivity[[i]][n_group+1],
                                Group= "Macro",
                                AUC=svm.nnet.multiroc$AUC[[i]][n_group+1],
                                Method = unique(svm.nnet.multiroc$Methods)[i])
      temp_data_3 <- data.frame(Specificity=svm.nnet.multiroc$Specificity[[i]][n_group+2],
                                Sensitivity=svm.nnet.multiroc$Sensitivity[[i]][n_group+2],
                                Group= "Micro",
                                AUC=svm.nnet.multiroc$AUC[[i]][n_group+2],
                                Method = unique(svm.nnet.multiroc$Methods)[i])
      colnames(temp_data_2) <- c("Specificity", "Sensitivity", "Group", "AUC", "Method")
      colnames(temp_data_3) <- c("Specificity", "Sensitivity", "Group", "AUC", "Method")
      res_df <- rbind(res_df, temp_data_2)
      res_df <- rbind(res_df, temp_data_3)
}

ggplot2::ggplot(res_df, ggplot2::aes(x = 1-Specificity, y=Sensitivity)) + ggplot2::geom_path(ggplot2::aes(color = Group, linetype=Method)) + ggplot2::geom_segment(ggplot2::aes(x = 0, y = 0, xend = 1, yend = 1), colour='grey', linetype = 'dotdash') + ggplot2::theme_bw() + ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5), legend.justification=c(1, 0), legend.position=c(.95, .05), legend.title=ggplot2::element_blank(), legend.background = ggplot2::element_rect(fill=NULL, size=0.5, linetype="solid", colour ="black"))
```

## Calcolo metriche
```{r}
source("compute_f_scores.R")

weights = list()
for (cls in class.names){
  count = class.frequencies[class.frequencies$class == cls,"frequencies"]
  weights[[cls]] = count
}
total_instances_count = Reduce('+', weights)
weights = mapply('/', weights, total_instances_count)
fscores = compute_f_scores(class.names, svm.tuned.validation.confusion_matrix, weights)

svm.macro.fscore = Reduce('+', fscores$fscores) / length(class.names)
svm.micro.fscore = Reduce('+', fscores$fscores.weighted)

sprintf("SVM macro F-Score: %1.3f", svm.macro.fscore)
sprintf("SVM micro F-Score: %1.3f", svm.micro.fscore)

per_class_metrics = data.frame("Precision" = unname(unlist(fscores$precisions)), "Recall" = unname(unlist(fscores$recalls)), "Fscore" = unname(unlist(fscores$fscores)))
rownames(per_class_metrics) = names(fscores$precisions)
per_class_metrics$Precision = per_class_metrics$Precision * 100
per_class_metrics$Recall = per_class_metrics$Recall * 100
print(per_class_metrics)
```

## Clustering

```{r}
trainset.clustering = dataset.input.pca
labels = dataset.pca[1]

# dataframe to vector conversion
labels.list <- c()
for(i in 1:nrow(labels)){
  if(labels[i,1] == labels.names[1]) labels.list = append(labels.list, 1)
  if(labels[i,1] == labels.names[2]) labels.list = append(labels.list, 2)
  if(labels[i,1] == labels.names[3]) labels.list = append(labels.list, 3)
  if(labels[i,1] == labels.names[4]) labels.list = append(labels.list, 4)
  if(labels[i,1] == labels.names[5]) labels.list = append(labels.list, 5)
  if(labels[i,1] == labels.names[6]) labels.list = append(labels.list, 6)
}

```

```{r}
set.seed(1)
nk = 2:12
SW = sapply(nk, function(k){cluster.stats(dist(trainset.clustering), 
                              kmeans(trainset.clustering, centers=k, nstart=5)$cluster)$avg.silwidth})
#SW
plot(nk, SW, type="l", xlab="number of clusters", ylab="average silhouette")
```

### K=6
Pur ottenendo una misura minore di Silhouette in confronto a k=3, e' stato comunque ritenuto opportuno analizzare i risultati ottenuti tramite l'esecuzioe con parametro k=6, che rappresenta il numero delle classi, e di conseguenza porta ad una maggiore misura di similarita' tra i cluster rilevati da k-means e l'effettiva suddivisione in classi del dataset
```{r}
fit = kmeans(trainset.clustering, 6, nstart=5)

#Silhouette
kms = silhouette(fit$cluster, dist(trainset.clustering))
plot(kms, col=1:6, border=NA, main = "Silhouette Plot for k=6")

# Dissimilarity Matrix
dissplot(dist(trainset.clustering), labels=fit$cluster, options=list(main="Kmeans clustering with k=6"))
```

```{r}
# Evaluation
sprintf("Similarity with (k=6): %f", cluster.evaluation(labels.list, fit$cluster))
```

```{r}
# visualization
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(1,2), main="Dimensions: 1 & 2", 
             geom="point")
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(1,3), main="Dimensions: 1 & 3", 
             geom="point")
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(1,4), main="Dimensions: 1 & 4", 
             geom="point")

fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(2,3), main="Dimensions: 2 & 3", 
             geom="point")
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(2,4), main="Dimensions: 2 & 4", 
             geom="point")

fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(3,4), main="Dimensions: 3 & 4", 
             geom="point")
```

```{r}
# Confusion Matrix
source("clustering_confusion_matrix.R")

confusion_matrix = clustering_confusion_matrix(fit$cluster, labels.list, 6, 6)

for (i in 1:6) {
    pie_title = sprintf("Cluster %d Distribution", i)
    pie(confusion_matrix[i,], main=pie_title, labels=class.names, col=rainbow(6))
}
```

```{r}
# 
for (i in 1:6) {
    pie_title = sprintf("Class %s Distribution", class.names[i])
    pie(confusion_matrix[,i], main=pie_title, labels=1:6, col=rainbow(6))
}
```

### K=3
Nonostante il valore di Silhouette maggiore rispetto a k=6, il parametro k=3 porta ad un peggioramento nella misura di similarita' tra cluster rilevati e classi del dataset
```{r}
fit = kmeans(trainset.clustering, 3, nstart=5)

kms = silhouette(fit$cluster, dist(trainset.clustering))
plot(kms, col=1:3, border=NA, main = "Silhouette Plot for k=3")

# Dissimilarity Matrix
dissplot(dist(trainset.clustering), labels=fit$cluster, options=list(main="Kmeans clustering with k=3"))
```

```{r}
# Evaluation
sprintf("Similarity with (k=3): %f", cluster.evaluation(labels.list, fit$cluster))
```

```{r}
# visualization
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(1,2), main="Dimensions: 1 & 2", 
             geom="point")
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(1,3), main="Dimensions: 1 & 3", 
             geom="point")
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(1,4), main="Dimensions: 1 & 4", 
             geom="point")

fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(2,3), main="Dimensions: 2 & 3", 
             geom="point")
fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(2,4), main="Dimensions: 2 & 4", 
             geom="point")

fviz_cluster(fit, trainset.clustering, ellipse.type = "norm", axes=c(3,4), main="Dimensions: 3 & 4", 
             geom="point")
```

```{r}
# Confusion Matrix
source("clustering_confusion_matrix.R")

confusion_matrix = clustering_confusion_matrix(fit$cluster, labels.list, 3, 6)

for (i in 1:3) {
    pie_title = sprintf("Cluster %d Disribution", i)
    pie(confusion_matrix[i,], main=pie_title, labels=class.names, col=rainbow(6))
}
```

```{r}
# 
for (i in 1:6) {
    pie_title = sprintf("Class %s Distribution", class.names[i])
    pie(confusion_matrix[,i], main=pie_title, labels=1:6, col=rainbow(3))
}
```

### Evaluation al variare di k
Evidenzia la misura di similarita' tra i cluster e le classi del dataset, al variaaare del parametro k
Il valore piu' alto e' ottenuto con k=6, in linea con le rilevazioni precedenti
```{r}
similarity = c()
for (k in 2:6) {
    fit = kmeans(trainset.clustering, k, nstart = 5)
    eval = cluster.evaluation(labels.list, fit$cluster)
    similarity = append(similarity, eval)
}
plot(2:6, similarity, type="b", main = "Similarity plot for ascending K", xlab="K")
```

```{r}
library(rpart);
decisionTree = rpart(class ~ Dim.1 + Dim.2 + Dim.3 + Dim.4, data = trainset, method = "class", control = rpart.control(cp = 1e-3))
decisionTree.prediction = predict(decisionTree, validationset, type = "class")
decisionTree.conf = table(decisionTree.prediction, validationset[,c("class")])
decisionTree.conf
acc = sum(diag(decisionTree.conf))/sum(decisionTree.conf)
plotcp(decisionTree)
```


```{r}
prunedTree = prune(decisionTree, cp = .02)
prunedTree.prediction = predict(prunedTree, validationset, type = "class")
prunedTree.conf = table(prunedTree.prediction, validationset[,c("class")])
prunedTree.conf
acc = sum(diag(prunedTree.conf))/sum(prunedTree.conf)

```

```{r}
fancyRpartPlot(decisionTree)
fancyRpartPlot(prunedTree)
```

```{r}
fscores = compute_f_scores(class.names, nnet.tuned.validation.confusion_matrix, weights)
nnet.macro.fscore = Reduce('+', fscores$fscores) / length(class.names)
nnet.micro.fscore = Reduce('+', fscores$fscores.weighted)
sprintf("NNet macro F-Score: %1.3f", nnet.macro.fscore)
sprintf("Net micro F-Score: %1.3f", nnet.micro.fscore)

per_class_metrics = data.frame("Precision" = unname(unlist(fscores$precisions)), "Recall" = unname(unlist(fscores$recalls)), "Fscore" = unname(unlist(fscores$fscores)))
rownames(per_class_metrics) = names(fscores$precisions)
per_class_metrics$Precision = per_class_metrics$Precision * 100
per_class_metrics$Recall = per_class_metrics$Recall * 100
print(per_class_metrics)
```
